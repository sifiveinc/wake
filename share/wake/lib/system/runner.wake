# Copyright 2024 SiFive, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You should have received a copy of LICENSE.Apache2 along with
# this software. If not, you may obtain a copy at
#
#    https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

package wake

from remote_cache import rscRunner makeRemoteCacheApi

# Unknown quantities are 0
export tuple Usage =
    export Status: Integer
    export Runtime: Double
    export CPUtime: Double
    export MemBytes: Integer
    export InBytes: Integer
    export OutBytes: Integer

export def emptyUsage =
    Usage 0 0.0 0.0 0 0 0

export def getUsageThreads (Usage _ run cpu _ _ _: Usage): Double =
    if run ==. 0.0 then
        cpu
    else
        cpu /. run

# RunnerInput is a subset of the fields supplied in the execution Plan
export tuple RunnerInput =
    export Label: String
    export Command: List String
    export Visible: List Path
    export Environment: List String
    export Directory: String
    export Stdin: String
    export Resources: List String
    # A unique prefix for this job
    export Prefix: String
    # Previous resource usage
    export Record: Usage
    # Determines if job should run in psuedoterminal
    export IsAtty: Boolean
    # Modify the Runner's reported inputs (files read).
    # Must be called exactly once for a given job.
    #
    # FnInputs may only be called once as the user provided function may not be idempotent.
    # Ex: FnInputs = tail being called multiple times would drop an extra file each time its called
    #     while the user only intentded for the very first file to be dropped
    #
    # Note for wrapping runners:
    #     It is expected that a wrapping runner maintain this invariant. What that looks like will
    #     depend heavily on the wrapper and requires due diligence. A cache wrapper for example
    #     should pass it along unmodified since it doesn't care about the actual inputs while
    #     a smart wrapper tracking file reads must pass a no-op function to the inner runner
    #     so that it can apply filtering after collecting the files read.
    export FnInputs: (List String => List String)
    # Modify the Runner's reported outputs (files created).
    # Must be called exactly once for a given job.
    #
    # FnOutputs may only be called once as the user provided function may not be idempotent.
    # Ex: FnOutputs = tail being called multiple times would drop an extra file each time its called
    #     while the user only intentded for the very first file to be dropped
    #
    # Note for wrapping runners:
    #     It is expected that a wrapping runner maintain this invariant. What that looks like will
    #     depend heavily on the wrapper and requires due diligence. A cache wrapper for example
    #     should pass it along unmodified since it doesn't care about the actual outputs while
    #     a smart wrapper tracking file writes must pass a no-op function to the inner runner
    #     so that it can apply filtering after collecting the files written.
    export FnOutputs: (List String => List String)

# The result returned by a Runner after running a job
export tuple RunnerOutput =
    # The filtered list of files actually read by the job. It can be used for more precise reruns
    export Inputs: List String
    # The filtered list of files output by the job. These files are hashed and elevated to Path
    export Outputs: List String
    # The list of files output by the job that should be deleted by wake --clean. May include files
    # listed in Outputs as they will be removed later. The files in the list that are not in
    # Outputs will not be hashed, published, or elevated to Path but will be deleted by wake --clean
    export CleanableOutputs: List String
    # The actual resource usage of the job that will be written to the database
    export Usage: Usage
    # If the runner failed before or after launching the job, this will contain the error message
    # as a status *separate* from the job's exit status.  Since it's all happening inside Wake,
    # this can carry a more descriptive message than the standard numeric exit code -- however,
    # the contents of that message should very rarely be incorporated into control-flow logic as
    # it could very easily be unreproducible.
    export RunnerError: Option Error

# A Runner describes a way to invoke a Plan to get a Job
export tuple Runner =
    export Name: String
    Fn: Job => RunnerInput => RunnerOutput

# makeRunner: Hides some of the boiler plate required to create a runner
#
# This function requires very advanced wake experience and should be used with the greatest amount
# of caution. Callers must ensure at the very least that `run` calls primJobLaunch and one of the
# many job "wait" functions. Historically runners allowed dispatching to an arbirary "base" or
# "inner" runner. This significantly complicated the system and led to very unexpected interactions.
# It is recomennded that runners don't accept an "inner" runner and instead directly call the job
# primatives. If wrapping is unavoidable then the specific runner being wrapped should be named
# instead of accepting an arbitrary runner parameter.
#
# localRunner is a good reference implementation of the run function.
export def makeRunner (name: String) (run: Job => RunnerInput => RunnerOutput): Runner =
    Runner name run

# localRunner: A runner that provides no sandboxing protections and no file tracking.
#
# You must use Fn{Inputs,Outputs} to fill in this information for wake to maintain safety and reusability
# Advanced usage only, proceed with caution
export def localRunner: Runner =
    def run (job: Job) ((RunnerInput _ cmd vis env dir stdin _ _ predict isAtty fnInputs fnOutputs): RunnerInput): RunnerOutput =
        def jobKey = JobKey dir stdin env.implode cmd.implode 0 "" isAtty.booleanToInteger
        def _ = primJobLaunch job jobKey predict

        def fileInputs =
            vis
            | map getPathName
            | fnInputs

        # Caller needs to fill this in from nothing
        def cleanable = Nil
        def fileOutputs = fnOutputs cleanable

        def Pair usage runnerError = match job.getJobReality
            Fail err -> Pair emptyUsage (Some err)
            # Set the runner status to be a success (`None`) - it's the callers responsibility
            # to set the appropriate status message if the job that ran is an extension of a runner
            Pass reality -> Pair reality None

        # Wait for the job to complete
        def return _ = RunnerOutput fileInputs fileOutputs cleanable usage runnerError

        waitJobMerged return job

    makeRunner "local" run

# virtualRunner: A runner that immediatly marks the job as complete using the predicted usage
#
# This runner is useful for tracking a unit of work that is job like but not launched as a process
export def virtualRunner: Runner =
    def run (job: Job) ((RunnerInput _ _ vis _ _ _ _ _ predict _ fnInputs fnOutputs): RunnerInput): RunnerOutput =
        def _ = primJobVirtual job "" "" predict

        def fileInputs =
            vis
            | map getPathName
            | fnInputs

        # Caller needs to fill this in from nothing
        def cleanable = Nil
        def fileOutputs = fnOutputs cleanable

        def Pair usage runnerError = match job.getJobReality
            Fail err -> Pair emptyUsage (Some err)
            # Set the runner status to be a success (`None`) - it's the callers responsibility
            # to set the appropriate status message if the job that ran is an extension of a runner
            Pass reality -> Pair reality None

        # Wait for the job to complete
        def return _ = RunnerOutput fileInputs fileOutputs cleanable usage runnerError

        waitJobMerged return job

    makeRunner "virtual" run

# Implement FUSE-based Runner
export def fuseRunner: Runner =
    def fuse = "{wakePath}/wakebox"

    makeJSONRunnerPlan fuse
    | editJSONRunnerPlanExtraEnv (editEnvironment "DEBUG_FUSE_WAKE" (\_ getenv "DEBUG_FUSE_WAKE"))
    | makeJSONRunner

export def defaultRunner: Runner =
    require Some config = getenv "WAKE_REMOTE_CACHE"
    else
        require Some _ = getenv "WAKE_LOCAL_JOB_CACHE"
        else fuseRunner

        # The fuseRunner does not actully mount over / and instead uses the
        # the host root as the sandbox root. This means that wakeroot will
        # change from depending on where wake is being run from. As a hack
        # to work around this we abuse the fact that the fuseRunner only
        # works in relative paths to make different runs consistent. Ideally
        # you'd have a more complex sandbox that kept the wakeroot at a
        # consistent place across runs.
        mkJobCacheRunner (\_ Pass "") "/workspace" fuseRunner

    match (makeRemoteCacheApi config)
        Pass api -> rscRunner api
        Fail (Error why _) ->
            def _ =
                printlnLevel logError "Remote Cache requested, but unavailable. Continuing anyways. Why: '{why}'"

            fuseRunner

# A plan describing how to construct a JSONRunner
# RawScript: the path to the script to run jobs with
# ExtraArgs: extra arguments to pass to ``RawScript``
# ExtraEnv: environment variables to pass to the script
# Estimate: predict local usage based on prior recorded usage
tuple JSONRunnerPlan =
    export RawScript: String
    export ExtraArgs: List String
    export ExtraEnv: List String
    export Estimate: Usage => Usage

# make a ``JSONRunnerPlan`` with ``Nil`` and ``(_)`` as defaults for ``ExtraArgs`` and ``Estimate`` respectively
# rawScript: String; the path to the script to run jobs with
export def makeJSONRunnerPlan (rawScript: String): JSONRunnerPlan =
    JSONRunnerPlan rawScript Nil Nil (_)

# Make a Runner that runs a named script to run jobs
# plan: JSONRunnerPlan; a tuple containing the arguments for this function
export def makeJSONRunner ((JSONRunnerPlan rawScript extraArgs extraEnv estimate): JSONRunnerPlan): Runner =
    def script = which (simplify rawScript)
    def executeOk = access script xOK

    def run (job: Job) ((RunnerInput label command visible environment directory stdin res prefix record isatty fnInputs fnOutputs): RunnerInput): RunnerOutput =
        require True = executeOk
        else
            def runnerError = "Runner {script} is not executable".makeError
            def _ = markJobSetupFailure job runnerError

            RunnerOutput Nil Nil Nil emptyUsage (Some runnerError)

        def Usage jobStatus runtime cputime membytes inbytes outbytes = record

        def json =
            JObject (
                "label" :-> JString label,
                "command" :-> command | map JString | JArray,
                "environment" :-> environment | map JString | JArray,
                "visible" :-> visible | map (_.getPathName.JString) | JArray,
                "directory" :-> JString directory,
                "stdin" :-> JString stdin,
                "resources" :-> res | map JString | JArray,
                "version" :-> JString version,
                "isolate-network" :-> JBoolean False,
                "isolate-pids" :-> JBoolean False,
                "mount-ops" :-> JArray (JObject ("type" :-> JString "workspace", "destination" :-> JString ".", Nil), Nil),
                "usage" :-> JObject (
                    "status" :-> JInteger jobStatus,
                    "runtime" :-> JDouble runtime,
                    "cputime" :-> JDouble cputime,
                    "membytes" :-> JInteger membytes,
                    "inbytes" :-> JInteger inbytes,
                    "outbytes" :-> JInteger outbytes,
                    Nil
                ),
                Nil
            )

        def buildDirName = ".build"
        def specFile = "{buildDirName}/spec-{prefix}.json"
        def resultFile = "{buildDirName}/result-{prefix}.json"

        def writeResult =
            write specFile (prettyJSON json)
            | addErrorContext "Failed to 'write {specFile}: '"
            | rmapFail (markJobSetupFailure job)

        require Pass _ = writeResult
        else RunnerOutput Nil Nil Nil emptyUsage writeResult.getFail

        def cmd = script, "-I", "-p", specFile, "-o", resultFile, extraArgs

        # Rewrite input so that the local runner can run the job with a configured sandbox
        # The identity function is passsed to the file input/output filters so no info is lost
        def localInput =
            RunnerInput
            label
            cmd
            Nil
            (extraEnv ++ environment)
            "."
            ""
            Nil
            prefix
            (estimate record)
            isatty
            identity
            identity

        # Dispatch to the local runner via composition and get the outputs
        def (Runner _ localRun) = localRunner
        def localOutput = localRun job localInput

        # The `localRunner` output reflects the status of the runner itself, not the job, and so we
        # need to save that separately and extract the job status from the result file.
        # In the failure case, though, the runner's usage might provide a slightly better fallback
        # for resource usage than simply returning all zeros.
        def runnerUsage = localOutput.getRunnerOutputUsage
        def runnerStatusCode = runnerUsage.getUsageStatus

        require 0 = runnerStatusCode
        else
            Some "Non-zero exit status ({str runnerStatusCode}) for JSON runner on {specFile}".makeError
            | RunnerOutput Nil Nil (specFile, Nil) runnerUsage

        require Pass content = parseJSONFile (Path resultFile "BadHash")
        else
            Some "Failed to parse output {resultFile} for JSON runner".makeError
            | RunnerOutput Nil Nil (specFile, Nil) runnerUsage

        def field name = match _ _
            _ (Fail f) -> Fail f
            None (Pass fn) -> Fail "{script} produced {resultFile}, which is missing usage/{name}"
            (Some x) (Pass fn) -> Pass (fn x)

        def jsonUsage = content // `usage`

        def jobUsageResult =
            Pass (Usage _ _ _ _ _ _)
            | field "status" (jsonUsage // `status` | getJInteger)
            | field "runtime" (jsonUsage // `runtime` | getJDouble)
            | field "cputime" (jsonUsage // `cputime` | getJDouble)
            | field "membytes" (jsonUsage // `membytes` | getJInteger)
            | field "inbytes" (jsonUsage // `inbytes` | getJInteger)
            | field "outbytes" (jsonUsage // `outbytes` | getJInteger)

        def getK exp =
            content // exp
            | getJArray
            | getOrElse Nil
            | mapPartial getJString

        def jsonInputs = getK `inputs`
        def jsonOutputs = getK `outputs`
        def filteredInputs = fnInputs jsonInputs
        def filteredOutputs = fnOutputs jsonOutputs
        def cleanableOutputs = specFile, resultFile, jsonOutputs

        match jobUsageResult
            Pass jobUsage ->
                None
                | RunnerOutput filteredInputs filteredOutputs cleanableOutputs jobUsage
            Fail msg ->
                Some msg.makeError
                | RunnerOutput filteredInputs filteredOutputs cleanableOutputs runnerUsage

    makeRunner "json-{script}" run
